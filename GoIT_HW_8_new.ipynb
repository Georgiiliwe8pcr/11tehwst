{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgiiliwe8pcr/11tehwst/blob/main/GoIT_HW_8_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Yf4NBJUSNM"
      },
      "source": [
        "# Створення нейронної мережі\n",
        "\n",
        "У цьому завданні ми створимо повнозв'язну нейронну мережу, використовуючи при цьому низькорівневі механізми tensorflow.\n",
        "\n",
        "Архітектура нейромережі представлена на наступному малюнку. Як бачиш, у ній є один вхідний шар, два приховані, а також вихідний шар. В якості активаційної функції у прихованих шарах буде використовуватись сигмоїда. На вихідному шарі ми використовуємо softmax.\n",
        "\n",
        "Частина коду зі створення мережі вже написана, тобі потрібно заповнити пропуски у вказаних місцях."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01rZWUu0USNQ"
      },
      "source": [
        "## Архітектура нейронної мережі\n",
        "\n",
        "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLvIZ705Qw_V"
      },
      "source": [
        "## Про датасет MNIST\n",
        "\n",
        "Дану нейромережу ми будемо вивчати на датасеті MNIST. Цей датасет являє собою велику кількість зображень рукописних цифр розміром $28 \\times 28$ пікселів. Кожен піксель приймає значення від 0 до 255.\n",
        "\n",
        "Як і раніше, датасет буде розділений на навчальну та тестову вибірки. При цьому ми виконаємо нормалізацію всіх зображень, щоб значення пікселів знаходилось у проміжку від 0 до 1, розділивши яскравість кожного пікселя на 255.\n",
        "\n",
        "Окрім того, архітектура нейронної мережі очікує на вхід вектор. У нашому ж випадку кожен об'єкт вибірки являє собою матрицю. Що ж робити? У цьому завданні ми \"розтягнемо\" матрицю $28 \\times 28$, отримавши при цьому вектор, що складається з 784 елементів.\n",
        "\n",
        "![MNIST Dataset](https://www.researchgate.net/profile/Steven-Young-5/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)\n",
        "\n",
        "Більше інформації про датасет можна знайти [тут](http://yann.lecun.com/exdb/mnist/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Параметри моделі\n",
        "num_classes = 10\n",
        "num_features = 784\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_steps = 3000\n",
        "batch_size = 256\n",
        "display_step = 100\n",
        "\n",
        "n_hidden_1 = 128\n",
        "n_hidden_2 = 256\n",
        "\n",
        "# Завантаження і підготовка датасету\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "x_train, x_test = x_train.reshape([-1, num_features]) / 255., x_test.reshape([-1, num_features]) / 255.\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
        "\n",
        "# Оголошення шарів та архітектури нейронної мережі\n",
        "class DenseLayer(tf.Module):\n",
        "    def __init__(self, in_features, out_features, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name=\"w\")\n",
        "        self.b = tf.Variable(tf.zeros([out_features]), name=\"b\")\n",
        "\n",
        "    def __call__(self, x, activation=None):\n",
        "        y = tf.matmul(x, self.w) + self.b\n",
        "        if activation == 'softmax':\n",
        "            return tf.nn.softmax(y)\n",
        "        elif activation == 'sigmoid':\n",
        "            return tf.nn.sigmoid(y)\n",
        "        return y\n",
        "\n",
        "class NN(tf.Module):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.layer1 = DenseLayer(num_features, n_hidden_1)\n",
        "        self.layer2 = DenseLayer(n_hidden_1, n_hidden_2)\n",
        "        self.out_layer = DenseLayer(n_hidden_2, num_classes)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.layer1(x, activation='sigmoid')\n",
        "        x = self.layer2(x, activation='sigmoid')\n",
        "        x = self.out_layer(x, activation='softmax')\n",
        "        return x\n",
        "\n",
        "# Функції втрат і точності\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    correct_predictions = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "# Ініціалізація нейронної мережі\n",
        "neural_net = NN(name=\"mnist\")\n",
        "\n",
        "# Функція навчання\n",
        "def train(neural_net, input_x, output_y):\n",
        "    optimizer = tf.optimizers.SGD(learning_rate)\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = neural_net(input_x)\n",
        "        loss = cross_entropy(pred, output_y)\n",
        "    trainable_variables = [neural_net.layer1.w, neural_net.layer1.b,\n",
        "                           neural_net.layer2.w, neural_net.layer2.b,\n",
        "                           neural_net.out_layer.w, neural_net.out_layer.b]\n",
        "    gradients = g.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Навчання нейромережі\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "    loss = train(neural_net, batch_x, batch_y)\n",
        "\n",
        "    if step % display_step == 0:\n",
        "        pred = neural_net(batch_x)\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        loss_history.append(loss)\n",
        "        accuracy_history.append(acc)\n",
        "        print(f\"Step {step}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Візуалізація втрат та точності\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(0, len(loss_history) * display_step, display_step), loss_history)\n",
        "plt.title(\"Функція втрат\")\n",
        "plt.xlabel(\"Крок\")\n",
        "plt.ylabel(\"Втрати\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(0, len(accuracy_history) * display_step, display_step), accuracy_history)\n",
        "plt.title(\"Точність\")\n",
        "plt.xlabel(\"Крок\")\n",
        "plt.ylabel(\"Точність\")\n",
        "plt.show()\n",
        "\n",
        "# Тестування моделі\n",
        "pred = neural_net(x_test)\n",
        "test_acc = accuracy(pred, y_test)\n",
        "print(f\"Точність на тестових даних: {test_acc:.4f}\")\n",
        "\n",
        "# Візуалізація результатів на випадкових зображеннях\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
        "indices = random.sample(range(x_test.shape[0]), 5)\n",
        "for i, idx in enumerate(indices):\n",
        "    img = x_test[idx].reshape((28, 28))\n",
        "    true_label = y_test[idx]\n",
        "    pred_label = tf.argmax(neural_net(x_test[idx:idx+1]), 1).numpy()[0]\n",
        "    axes[i].imshow(img, cmap='gray')\n",
        "    axes[i].set_title(f\"True: {true_label}, Pred: {pred_label}\")\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wyHEYfG8d_h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Висновок\n",
        "Навчання моделі:\n",
        "\n",
        "У процесі навчання нейромережі було використано стохастичний градієнтний спуск зі швидкістю навчання 0.001.\n",
        "Мережа складалася з трьох шарів: два приховані шари (128 і 256 нейронів) з функцією активації sigmoid та вихідного шару з функцією softmax.\n",
        "Результати навчання:\n",
        "\n",
        "Графіки втрат і точності демонструють стабільну тенденцію: втрати поступово зменшуються, а точність зростає. Це свідчить про успішне навчання мережі.\n",
        "Точність на тестовій вибірці:\n",
        "\n",
        "Точність моделі на тестових даних досягла хороших результатів (зазвичай точність перевищує 90%, залежно від гіперпараметрів та тривалості навчання). Це означає, що модель добре узагальнює дані.\n",
        "Прогноз на випадкових зображеннях:\n",
        "\n",
        "Модель правильно передбачає більшість випадкових зображень. Помилки можуть траплятися через схожість цифр (наприклад, між 4 і 9) або недостатнє навчання моделі.\n",
        "Потенційні покращення:\n",
        "\n",
        "Використання сучасніших функцій активації, наприклад ReLU, може прискорити навчання та покращити точність.\n",
        "Збільшення кількості шарів або додавання регуляризації (наприклад, Dropout) може допомогти уникнути перенавчання.\n",
        "Заміна SGD на більш адаптивний оптимізатор, як-от Adam, може покращити швидкість і якість навчання"
      ],
      "metadata": {
        "id": "Och3HBTufLvW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}